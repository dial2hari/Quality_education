{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7148e72c",
   "metadata": {
    "papermill": {
     "duration": 0.019986,
     "end_time": "2023-02-24T11:05:00.701718",
     "exception": false,
     "start_time": "2023-02-24T11:05:00.681732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32fae418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly==5.14.1 in c:\\users\\86177\\anaconda3\\lib\\site-packages (5.14.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\86177\\anaconda3\\lib\\site-packages (from plotly==5.14.1) (8.2.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\86177\\anaconda3\\lib\\site-packages (from plotly==5.14.1) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\86177\\anaconda3\\lib\\site-packages (from packaging->plotly==5.14.1) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install plotly==5.14.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7bb684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\86177\\anaconda3\\lib\\site-packages (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0aea82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:00.738057Z",
     "iopub.status.busy": "2023-02-24T11:05:00.737311Z",
     "iopub.status.idle": "2023-02-24T11:05:04.497570Z",
     "shell.execute_reply": "2023-02-24T11:05:04.496088Z"
    },
    "papermill": {
     "duration": 3.781166,
     "end_time": "2023-02-24T11:05:04.500095",
     "exception": false,
     "start_time": "2023-02-24T11:05:00.718929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\86177\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b2eb0",
   "metadata": {
    "papermill": {
     "duration": 0.019001,
     "end_time": "2023-02-24T11:05:04.535715",
     "exception": false,
     "start_time": "2023-02-24T11:05:04.516714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ff3e10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:04.576114Z",
     "iopub.status.busy": "2023-02-24T11:05:04.575724Z",
     "iopub.status.idle": "2023-02-24T11:05:04.850704Z",
     "shell.execute_reply": "2023-02-24T11:05:04.848828Z"
    },
    "papermill": {
     "duration": 0.297384,
     "end_time": "2023-02-24T11:05:04.853530",
     "exception": false,
     "start_time": "2023-02-24T11:05:04.556146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('cyberbullying_tweets.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "190b7f46",
   "metadata": {
    "papermill": {
     "duration": 0.017539,
     "end_time": "2023-02-24T11:05:05.231347",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.213808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Renaming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72a4a89b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:05.269256Z",
     "iopub.status.busy": "2023-02-24T11:05:05.268914Z",
     "iopub.status.idle": "2023-02-24T11:05:05.275883Z",
     "shell.execute_reply": "2023-02-24T11:05:05.274573Z"
    },
    "papermill": {
     "duration": 0.029059,
     "end_time": "2023-02-24T11:05:05.278476",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.249417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.rename(columns={'tweet_text': 'text', 'cyberbullying_type': 'sentiment'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcce700",
   "metadata": {
    "papermill": {
     "duration": 0.018283,
     "end_time": "2023-02-24T11:05:05.366670",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.348387",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Adding Encoded column for sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adbad88b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:05.404643Z",
     "iopub.status.busy": "2023-02-24T11:05:05.404290Z",
     "iopub.status.idle": "2023-02-24T11:05:05.429566Z",
     "shell.execute_reply": "2023-02-24T11:05:05.428251Z"
    },
    "papermill": {
     "duration": 0.047189,
     "end_time": "2023-02-24T11:05:05.432010",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.384821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[\"sentiment_encoded\"] = data['sentiment'].replace({\"religion\": 1, \"age\": 2, \"ethnicity\": 3, \"gender\": 4, \"other_cyberbullying\": 5,\"not_cyberbullying\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a6d1bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:05.521517Z",
     "iopub.status.busy": "2023-02-24T11:05:05.520908Z",
     "iopub.status.idle": "2023-02-24T11:05:05.531994Z",
     "shell.execute_reply": "2023-02-24T11:05:05.530938Z"
    },
    "papermill": {
     "duration": 0.032196,
     "end_time": "2023-02-24T11:05:05.534042",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.501846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc3e7c",
   "metadata": {
    "papermill": {
     "duration": 0.01749,
     "end_time": "2023-02-24T11:05:05.569557",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.552067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e92d57",
   "metadata": {
    "papermill": {
     "duration": 0.017292,
     "end_time": "2023-02-24T11:05:05.605407",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.588115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function to Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "105bd303",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:05.643142Z",
     "iopub.status.busy": "2023-02-24T11:05:05.642799Z",
     "iopub.status.idle": "2023-02-24T11:05:05.646864Z",
     "shell.execute_reply": "2023-02-24T11:05:05.646003Z"
    },
    "papermill": {
     "duration": 0.024838,
     "end_time": "2023-02-24T11:05:05.648759",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.623921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def strip_emoji(text):\n",
    "    return emoji.replace_emoji(text,replace=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402282f5",
   "metadata": {
    "papermill": {
     "duration": 0.018464,
     "end_time": "2023-02-24T11:05:05.685623",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.667159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fucntion to Convert text to lowercase, remove (/r, /n  characters), URLs, non-utf characters, Numbers, punctuations,stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1af708a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:05.723216Z",
     "iopub.status.busy": "2023-02-24T11:05:05.722875Z",
     "iopub.status.idle": "2023-02-24T11:05:05.729449Z",
     "shell.execute_reply": "2023-02-24T11:05:05.728784Z"
    },
    "papermill": {
     "duration": 0.027591,
     "end_time": "2023-02-24T11:05:05.730966",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.703375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def strip_all_entities(text): \n",
    "    text = text.replace('\\r', '').replace('\\n', ' ').lower()\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text)\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "    text = re.sub(r'(.)1+', r'1', text)\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    stopchars= string.punctuation\n",
    "    table = str.maketrans('', '', stopchars)\n",
    "    text = text.translate(table)\n",
    "    text = [word for word in text.split() if word not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a1ea1b",
   "metadata": {
    "papermill": {
     "duration": 0.018845,
     "end_time": "2023-02-24T11:05:05.768184",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.749339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function to remove contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9104a077",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:05.806921Z",
     "iopub.status.busy": "2023-02-24T11:05:05.806339Z",
     "iopub.status.idle": "2023-02-24T11:05:05.811957Z",
     "shell.execute_reply": "2023-02-24T11:05:05.811300Z"
    },
    "papermill": {
     "duration": 0.027621,
     "end_time": "2023-02-24T11:05:05.814113",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.786492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decontract(text):\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea684b42",
   "metadata": {
    "papermill": {
     "duration": 0.018338,
     "end_time": "2023-02-24T11:05:05.852840",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.834502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function to Clean Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b516dad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:05.892691Z",
     "iopub.status.busy": "2023-02-24T11:05:05.891896Z",
     "iopub.status.idle": "2023-02-24T11:05:05.897220Z",
     "shell.execute_reply": "2023-02-24T11:05:05.896512Z"
    },
    "papermill": {
     "duration": 0.028297,
     "end_time": "2023-02-24T11:05:05.899498",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.871201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_hashtags(tweet):\n",
    "    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet))\n",
    "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet))\n",
    "    return new_tweet2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacb44cd",
   "metadata": {
    "papermill": {
     "duration": 0.017479,
     "end_time": "2023-02-24T11:05:05.935334",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.917855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function to Filter Special Characters such as $, &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41bc0a8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:05.972504Z",
     "iopub.status.busy": "2023-02-24T11:05:05.971937Z",
     "iopub.status.idle": "2023-02-24T11:05:05.977753Z",
     "shell.execute_reply": "2023-02-24T11:05:05.976762Z"
    },
    "papermill": {
     "duration": 0.027139,
     "end_time": "2023-02-24T11:05:05.979917",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.952778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_chars(a):\n",
    "    sent = []\n",
    "    for word in a.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    return ' '.join(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a5a23",
   "metadata": {
    "papermill": {
     "duration": 0.017457,
     "end_time": "2023-02-24T11:05:06.016389",
     "exception": false,
     "start_time": "2023-02-24T11:05:05.998932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function to remove mutiple sequence spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5563141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:06.054898Z",
     "iopub.status.busy": "2023-02-24T11:05:06.053777Z",
     "iopub.status.idle": "2023-02-24T11:05:06.058774Z",
     "shell.execute_reply": "2023-02-24T11:05:06.057700Z"
    },
    "papermill": {
     "duration": 0.026434,
     "end_time": "2023-02-24T11:05:06.060671",
     "exception": false,
     "start_time": "2023-02-24T11:05:06.034237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_mult_spaces(text):\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b21e69",
   "metadata": {
    "papermill": {
     "duration": 0.017665,
     "end_time": "2023-02-24T11:05:06.096802",
     "exception": false,
     "start_time": "2023-02-24T11:05:06.079137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function to apply stemming to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc10a899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:06.135145Z",
     "iopub.status.busy": "2023-02-24T11:05:06.134769Z",
     "iopub.status.idle": "2023-02-24T11:05:06.140677Z",
     "shell.execute_reply": "2023-02-24T11:05:06.139012Z"
    },
    "papermill": {
     "duration": 0.028167,
     "end_time": "2023-02-24T11:05:06.143467",
     "exception": false,
     "start_time": "2023-02-24T11:05:06.115300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    ps = PorterStemmer()\n",
    "    return ' '.join([ps.stem(words) for words in tokenized])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bcbdd6",
   "metadata": {
    "papermill": {
     "duration": 0.018789,
     "end_time": "2023-02-24T11:05:06.181626",
     "exception": false,
     "start_time": "2023-02-24T11:05:06.162837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function to apply lemmatization to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a900911c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:06.222497Z",
     "iopub.status.busy": "2023-02-24T11:05:06.221145Z",
     "iopub.status.idle": "2023-02-24T11:05:06.227079Z",
     "shell.execute_reply": "2023-02-24T11:05:06.225665Z"
    },
    "papermill": {
     "duration": 0.028765,
     "end_time": "2023-02-24T11:05:06.229363",
     "exception": false,
     "start_time": "2023-02-24T11:05:06.200598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    lm = WordNetLemmatizer()\n",
    "    return ' '.join([lm.lemmatize(words) for words in tokenized])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee820f",
   "metadata": {
    "papermill": {
     "duration": 0.018179,
     "end_time": "2023-02-24T11:05:06.266386",
     "exception": false,
     "start_time": "2023-02-24T11:05:06.248207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Function to Preprocess the text by applying all above functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb7c2e5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:06.305586Z",
     "iopub.status.busy": "2023-02-24T11:05:06.305199Z",
     "iopub.status.idle": "2023-02-24T11:05:06.311886Z",
     "shell.execute_reply": "2023-02-24T11:05:06.310611Z"
    },
    "papermill": {
     "duration": 0.028725,
     "end_time": "2023-02-24T11:05:06.313813",
     "exception": false,
     "start_time": "2023-02-24T11:05:06.285088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = strip_emoji(text)\n",
    "    text = decontract(text)\n",
    "    text = strip_all_entities(text)\n",
    "    text = clean_hashtags(text)\n",
    "    text = filter_chars(text)\n",
    "    text = remove_mult_spaces(text)\n",
    "    text = stemmer(text)\n",
    "    text = lemmatize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ad184ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df1):\n",
    "    df1['cleaned_text'] = df1['text'].apply(strip_emoji)\n",
    "    df1['cleaned_text'] = df1['text'].apply(decontract)\n",
    "    df1['cleaned_text'] = df1['text'].apply(strip_all_entities)\n",
    "    df1['cleaned_text'] = df1['text'].apply(clean_hashtags)\n",
    "    df1['cleaned_text'] = df1['text'].apply(filter_chars)\n",
    "    df1['cleaned_text'] = df1['text'].apply(remove_mult_spaces)\n",
    "    df1['cleaned_text'] = df1['text'].apply(stemmer)\n",
    "    df1['cleaned_text'] = df1['text'].apply(lemmatize)\n",
    "    return df1['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "776a299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "userinput = \"pussy pussy #$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f836f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        In other word # katandandre , your food wa cra...\n",
       "1        Why is # aussietv so white ? # MKR # theblock ...\n",
       "2        @ XochitlSuckkks a classy whore ? Or more red ...\n",
       "3        @ Jason_Gio meh . : P thanks for the head up ,...\n",
       "4        @ RudhoeEnglish This is an ISIS account preten...\n",
       "                               ...                        \n",
       "47687    Black ppl are n't expected to do anything , de...\n",
       "47688    Turner did not withhold his disappointment . T...\n",
       "47689    I swear to God . This dumb nigger bitch . I ha...\n",
       "47690    Yea fuck you RT @ therealexel : IF YOURE A NIG...\n",
       "47691    Bro . U got ta chill RT @ CHILLShrammy : Dog F...\n",
       "Name: cleaned_text, Length: 44650, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data11 = preprocess(data)\n",
    "data11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad052602",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:06.352978Z",
     "iopub.status.busy": "2023-02-24T11:05:06.352667Z",
     "iopub.status.idle": "2023-02-24T11:05:41.260519Z",
     "shell.execute_reply": "2023-02-24T11:05:41.259019Z"
    },
    "papermill": {
     "duration": 34.948031,
     "end_time": "2023-02-24T11:05:41.280632",
     "exception": false,
     "start_time": "2023-02-24T11:05:06.332601",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>word katandandr food crapilici mkr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>aussietv white mkr theblock imacelebrityau tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>classi whore red velvet cupcak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>meh p thank head concern anoth angri dude twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>isi account pretend kurdish account like islam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          sentiment  \\\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying   \n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying   \n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying   \n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying   \n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying   \n",
       "\n",
       "   sentiment_encoded                                       cleaned_text  \n",
       "0                  6                 word katandandr food crapilici mkr  \n",
       "1                  6  aussietv white mkr theblock imacelebrityau tod...  \n",
       "2                  6                     classi whore red velvet cupcak  \n",
       "3                  6  meh p thank head concern anoth angri dude twitter  \n",
       "4                  6  isi account pretend kurdish account like islam...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_text'] = data['text'].apply(preprocess)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d975f931",
   "metadata": {
    "papermill": {
     "duration": 0.018495,
     "end_time": "2023-02-24T11:05:41.355145",
     "exception": false,
     "start_time": "2023-02-24T11:05:41.336650",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dealing with Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b426b6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:41.453549Z",
     "iopub.status.busy": "2023-02-24T11:05:41.453129Z",
     "iopub.status.idle": "2023-02-24T11:05:41.493013Z",
     "shell.execute_reply": "2023-02-24T11:05:41.491459Z"
    },
    "papermill": {
     "duration": 0.062652,
     "end_time": "2023-02-24T11:05:41.495580",
     "exception": false,
     "start_time": "2023-02-24T11:05:41.432928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.drop_duplicates(\"cleaned_text\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53726c",
   "metadata": {
    "papermill": {
     "duration": 0.018609,
     "end_time": "2023-02-24T11:05:41.570434",
     "exception": false,
     "start_time": "2023-02-24T11:05:41.551825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d633b40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:41.610016Z",
     "iopub.status.busy": "2023-02-24T11:05:41.609717Z",
     "iopub.status.idle": "2023-02-24T11:05:46.586922Z",
     "shell.execute_reply": "2023-02-24T11:05:46.586115Z"
    },
    "papermill": {
     "duration": 5.000061,
     "end_time": "2023-02-24T11:05:46.589310",
     "exception": false,
     "start_time": "2023-02-24T11:05:41.589249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tweet_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>word katandandr food crapilici mkr</td>\n",
       "      <td>[word, katandandr, food, crapilici, mkr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>aussietv white mkr theblock imacelebrityau tod...</td>\n",
       "      <td>[aussietv, white, mkr, theblock, imacelebritya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>classi whore red velvet cupcak</td>\n",
       "      <td>[classi, whore, red, velvet, cupcak]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>meh p thank head concern anoth angri dude twitter</td>\n",
       "      <td>[meh, p, thank, head, concern, anoth, angri, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>isi account pretend kurdish account like islam...</td>\n",
       "      <td>[isi, account, pretend, kurdish, account, like...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          sentiment  \\\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying   \n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying   \n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying   \n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying   \n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying   \n",
       "\n",
       "   sentiment_encoded                                       cleaned_text  \\\n",
       "0                  6                 word katandandr food crapilici mkr   \n",
       "1                  6  aussietv white mkr theblock imacelebrityau tod...   \n",
       "2                  6                     classi whore red velvet cupcak   \n",
       "3                  6  meh p thank head concern anoth angri dude twitter   \n",
       "4                  6  isi account pretend kurdish account like islam...   \n",
       "\n",
       "                                          tweet_list  \n",
       "0           [word, katandandr, food, crapilici, mkr]  \n",
       "1  [aussietv, white, mkr, theblock, imacelebritya...  \n",
       "2               [classi, whore, red, velvet, cupcak]  \n",
       "3  [meh, p, thank, head, concern, anoth, angri, d...  \n",
       "4  [isi, account, pretend, kurdish, account, like...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tweet_list'] = data['cleaned_text'].apply(word_tokenize)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78485dd",
   "metadata": {
    "papermill": {
     "duration": 0.020869,
     "end_time": "2023-02-24T11:05:46.631090",
     "exception": false,
     "start_time": "2023-02-24T11:05:46.610221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Checking length of various tweet texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9e0a514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:46.674375Z",
     "iopub.status.busy": "2023-02-24T11:05:46.674038Z",
     "iopub.status.idle": "2023-02-24T11:05:46.704689Z",
     "shell.execute_reply": "2023-02-24T11:05:46.703540Z"
    },
    "papermill": {
     "duration": 0.054693,
     "end_time": "2023-02-24T11:05:46.707695",
     "exception": false,
     "start_time": "2023-02-24T11:05:46.653002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_len = []\n",
    "for text in data.tweet_list:\n",
    "    tweet_len = len(text)\n",
    "    text_len.append(tweet_len)\n",
    "data['text_len'] = text_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e0381",
   "metadata": {
    "papermill": {
     "duration": 0.019491,
     "end_time": "2023-02-24T11:05:47.620367",
     "exception": false,
     "start_time": "2023-02-24T11:05:47.600876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Removing text without words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "882ca26c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:47.661692Z",
     "iopub.status.busy": "2023-02-24T11:05:47.661323Z",
     "iopub.status.idle": "2023-02-24T11:05:47.680951Z",
     "shell.execute_reply": "2023-02-24T11:05:47.679586Z"
    },
    "papermill": {
     "duration": 0.043771,
     "end_time": "2023-02-24T11:05:47.683579",
     "exception": false,
     "start_time": "2023-02-24T11:05:47.639808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data[data['text_len']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdd70660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:47.726075Z",
     "iopub.status.busy": "2023-02-24T11:05:47.725573Z",
     "iopub.status.idle": "2023-02-24T11:05:47.731882Z",
     "shell.execute_reply": "2023-02-24T11:05:47.730371Z"
    },
    "papermill": {
     "duration": 0.030764,
     "end_time": "2023-02-24T11:05:47.734325",
     "exception": false,
     "start_time": "2023-02-24T11:05:47.703561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44650, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c791f39f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:59.283757Z",
     "iopub.status.busy": "2023-02-24T11:05:59.282973Z",
     "iopub.status.idle": "2023-02-24T11:05:59.296204Z",
     "shell.execute_reply": "2023-02-24T11:05:59.294498Z"
    },
    "papermill": {
     "duration": 0.057023,
     "end_time": "2023-02-24T11:05:59.298528",
     "exception": false,
     "start_time": "2023-02-24T11:05:59.241505",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tweet_list</th>\n",
       "      <th>text_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>word katandandr food crapilici mkr</td>\n",
       "      <td>[word, katandandr, food, crapilici, mkr]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>aussietv white mkr theblock imacelebrityau tod...</td>\n",
       "      <td>[aussietv, white, mkr, theblock, imacelebritya...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>classi whore red velvet cupcak</td>\n",
       "      <td>[classi, whore, red, velvet, cupcak]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>meh p thank head concern anoth angri dude twitter</td>\n",
       "      <td>[meh, p, thank, head, concern, anoth, angri, d...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>6</td>\n",
       "      <td>isi account pretend kurdish account like islam...</td>\n",
       "      <td>[isi, account, pretend, kurdish, account, like...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          sentiment  \\\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying   \n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying   \n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying   \n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying   \n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying   \n",
       "\n",
       "   sentiment_encoded                                       cleaned_text  \\\n",
       "0                  6                 word katandandr food crapilici mkr   \n",
       "1                  6  aussietv white mkr theblock imacelebrityau tod...   \n",
       "2                  6                     classi whore red velvet cupcak   \n",
       "3                  6  meh p thank head concern anoth angri dude twitter   \n",
       "4                  6  isi account pretend kurdish account like islam...   \n",
       "\n",
       "                                          tweet_list  text_len  \n",
       "0           [word, katandandr, food, crapilici, mkr]         5  \n",
       "1  [aussietv, white, mkr, theblock, imacelebritya...        11  \n",
       "2               [classi, whore, red, velvet, cupcak]         5  \n",
       "3  [meh, p, thank, head, concern, anoth, angri, d...         9  \n",
       "4  [isi, account, pretend, kurdish, account, like...         8  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4fc40868",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:59.379916Z",
     "iopub.status.busy": "2023-02-24T11:05:59.379352Z",
     "iopub.status.idle": "2023-02-24T11:05:59.385065Z",
     "shell.execute_reply": "2023-02-24T11:05:59.383642Z"
    },
    "papermill": {
     "duration": 0.048944,
     "end_time": "2023-02-24T11:05:59.387306",
     "exception": false,
     "start_time": "2023-02-24T11:05:59.338362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentiments = [\"religion\", \"age\", \"ethnicity\", \"gender\", \"other_cyberbullying\",\"not_cyberbullying\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6a9f6",
   "metadata": {
    "papermill": {
     "duration": 0.038685,
     "end_time": "2023-02-24T11:05:59.466094",
     "exception": false,
     "start_time": "2023-02-24T11:05:59.427409",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Splitting Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72b01a84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:59.550137Z",
     "iopub.status.busy": "2023-02-24T11:05:59.549081Z",
     "iopub.status.idle": "2023-02-24T11:05:59.556030Z",
     "shell.execute_reply": "2023-02-24T11:05:59.555358Z"
    },
    "papermill": {
     "duration": 0.052399,
     "end_time": "2023-02-24T11:05:59.557955",
     "exception": false,
     "start_time": "2023-02-24T11:05:59.505556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X,Y = data['cleaned_text'],data['sentiment_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43c48c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       word katandandr food crapilici mkr\n",
       "1        aussietv white mkr theblock imacelebrityau tod...\n",
       "2                           classi whore red velvet cupcak\n",
       "3        meh p thank head concern anoth angri dude twitter\n",
       "4        isi account pretend kurdish account like islam...\n",
       "                               ...                        \n",
       "47687    black ppl expect anyth depend anyth yet free p...\n",
       "47688    turner withhold disappoint turner call court a...\n",
       "47689    swear god dumb nigger bitch got bleach hair re...\n",
       "47690    yea fuck rt your nigger fuck unfollow fuck dum...\n",
       "47691    bro u got ta chill rt dog fuck kp dumb nigger ...\n",
       "Name: cleaned_text, Length: 44650, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "025ef933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:59.652013Z",
     "iopub.status.busy": "2023-02-24T11:05:59.651707Z",
     "iopub.status.idle": "2023-02-24T11:05:59.677412Z",
     "shell.execute_reply": "2023-02-24T11:05:59.676133Z"
    },
    "papermill": {
     "duration": 0.070171,
     "end_time": "2023-02-24T11:05:59.681003",
     "exception": false,
     "start_time": "2023-02-24T11:05:59.610832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31255,) (31255,) (13395,) (13395,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, stratify =Y, random_state = 42)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8131081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41596    realli whoever made pictur honestli go die dum...\n",
       "6263                                well support lost mine\n",
       "17071    look like daeshbag complet broken flee koban area\n",
       "21191                   vote prevent muslim genocid israel\n",
       "38088    exampl sunset shimmer villain mlp equestria gi...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ee65b",
   "metadata": {
    "papermill": {
     "duration": 0.040259,
     "end_time": "2023-02-24T11:05:59.762570",
     "exception": false,
     "start_time": "2023-02-24T11:05:59.722311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## tf-idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a878ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn2pmml import PMMLPipeline, sklearn2pmml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "033e90a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cleaned_text'], dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX = data[['cleaned_text']]\n",
    "\n",
    "XX.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "317122b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn2pmml.feature_extraction.text import Splitter\n",
    "vectorizer = TfidfVectorizer(analyzer = \"word\", token_pattern = None, tokenizer = Splitter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0e6331d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade nyoka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "185e68df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from nyoka import skl_to_pmml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "759ed915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780                           uu le hacemo bull laurita uu\n",
       "17323    u r spread venom hindu first check ur islam te...\n",
       "39189    girl use bulli middl school h show dream last ...\n",
       "19066    christian woman gay friend support sexual oppr...\n",
       "44698    personnel broadway sacramento california queer...\n",
       "                               ...                        \n",
       "42704    fuck nigger yourst date hope last forev supah ...\n",
       "32187    share bulli quirkynerd high school obviou bull...\n",
       "14041    worri bitch live mother even get place name go...\n",
       "33441    how girl gon na say bulli high school wasnt li...\n",
       "39289                    cancel she high school fuck bulli\n",
       "Name: cleaned_text, Length: 31255, dtype: object"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83eb20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = LinearSVC(C=1, loss='hinge')\n",
    "# here you can use the key classifier, if suitable\n",
    "pipeline = Pipeline([(\"vect\", vectorizer),(\"model\", mod) ])\n",
    "#training the model\n",
    "pipeline.fit(X_train,y_train)\n",
    "# exporting the model\n",
    "skl_to_pmml(pipeline,XX.columns,\"sentiment_encoded\",\"lsvc_tfidf.pmml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdde66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "288a5161",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<function preprocess at 0x00000217A5E9F700>' (type <class 'function'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13588/4112820728.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hinge'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# here you can use the key classifier, if suitable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'preprocess'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vect\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#training the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86177\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86177\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, steps, memory, verbose)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\86177\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_validate_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m             if (not (hasattr(t, \"fit\") or hasattr(t, \"fit_transform\")) or not\n\u001b[0;32m    167\u001b[0m                     hasattr(t, \"transform\")):\n\u001b[1;32m--> 168\u001b[1;33m                 raise TypeError(\"All intermediate steps should be \"\n\u001b[0m\u001b[0;32m    169\u001b[0m                                 \u001b[1;34m\"transformers and implement fit and transform \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                                 \u001b[1;34m\"or be the string 'passthrough' \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<function preprocess at 0x00000217A5E9F700>' (type <class 'function'>) doesn't"
     ]
    }
   ],
   "source": [
    "mod = LinearSVC(C=1, loss='hinge')\n",
    "# here you can use the key classifier, if suitable\n",
    "pipeline = Pipeline([('preprocess', preprocess),(\"vect\", vectorizer),(\"model\", mod) ])\n",
    "#training the model\n",
    "pipeline.fit(X_train,y_train)\n",
    "# exporting the model\n",
    "skl_to_pmml(pipeline,XX.columns,\"sentiment_encoded\",\"lsvc_tfidf.pmml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd9d4d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "58c2e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tf_idf = TfidfVectorizer()\n",
    "# pipeline = PMMLPipeline([ (\"vect\", tf_idf) ])\n",
    "# #training the model\n",
    "# pipeline.fit(X_train)\n",
    "# # exporting the model\n",
    "# sklearn2pmml(pipeline, 'tfidf.pmml', with_repr = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d385a422",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:05:59.844318Z",
     "iopub.status.busy": "2023-02-24T11:05:59.843495Z",
     "iopub.status.idle": "2023-02-24T11:06:00.508763Z",
     "shell.execute_reply": "2023-02-24T11:06:00.506439Z"
    },
    "papermill": {
     "duration": 0.709931,
     "end_time": "2023-02-24T11:06:00.512950",
     "exception": false,
     "start_time": "2023-02-24T11:05:59.803019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31255, 29111)\n",
      "(13395, 29111)\n"
     ]
    }
   ],
   "source": [
    "tf_idf = TfidfVectorizer()\n",
    "X_train_tf = tf_idf.fit_transform(X_train)\n",
    "X_test_tf = tf_idf.transform(X_test)\n",
    "print(X_train_tf.shape)\n",
    "print(X_test_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2bcac9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01bfc3b",
   "metadata": {
    "papermill": {
     "duration": 0.041916,
     "end_time": "2023-02-24T11:06:10.967935",
     "exception": false,
     "start_time": "2023-02-24T11:06:10.926019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c39ea5ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:06:11.054728Z",
     "iopub.status.busy": "2023-02-24T11:06:11.052611Z",
     "iopub.status.idle": "2023-02-24T11:06:11.059527Z",
     "shell.execute_reply": "2023-02-24T11:06:11.057979Z"
    },
    "papermill": {
     "duration": 0.053053,
     "end_time": "2023-02-24T11:06:11.062033",
     "exception": false,
     "start_time": "2023-02-24T11:06:11.008980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lin_svc = LinearSVC(C=1, loss='hinge')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b8771da",
   "metadata": {
    "papermill": {
     "duration": 0.041191,
     "end_time": "2023-02-24T11:10:40.327061",
     "exception": false,
     "start_time": "2023-02-24T11:10:40.285870",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "987a803d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-24T11:10:40.413234Z",
     "iopub.status.busy": "2023-02-24T11:10:40.412905Z",
     "iopub.status.idle": "2023-02-24T11:10:40.945492Z",
     "shell.execute_reply": "2023-02-24T11:10:40.944337Z"
    },
    "papermill": {
     "duration": 0.578445,
     "end_time": "2023-02-24T11:10:40.947792",
     "exception": false,
     "start_time": "2023-02-24T11:10:40.369347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "lin_svc.fit(X_train_tf,y_train)\n",
    "y_pred = lin_svc.predict(X_test_tf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "332858bb",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4f9f742b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>your are a faggot pussy pussy..</td>\n",
       "      <td>faggot pussi pussi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              text        cleaned_text\n",
       "0  your are a faggot pussy pussy..  faggot pussi pussi"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv('test.csv')\n",
    "data1 = data1.rename(columns={'tweet_text': 'text'})\n",
    "data1['cleaned_text'] = data1['text'].apply(preprocess)\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c4f93cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test11 = data1['cleaned_text']\n",
    "test11_tf = tf_idf.transform(test11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1abf47e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    faggot pussi pussi\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8e2feedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 20451)\t0.8815727952282691\n",
      "  (0, 8353)\t0.47204809788136637\n"
     ]
    }
   ],
   "source": [
    "print(test11_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0d9b650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = lin_svc.predict(test11_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2eee3149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 351.792294,
   "end_time": "2023-02-24T11:10:44.551011",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-24T11:04:52.758717",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
